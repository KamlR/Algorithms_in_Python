### Rolling hash
Объясню идею rolling hash относительно алгоритма rabin-karp, хотя это не единственное применение.
В алгоритме мы хотим за минимальное время находить все вхождения (или только первое, или просто проверять наличие указанной подстроки в строке) нашей подстроки в строке. Если решать самым тривиальным способом, то сложность получается O(n*m). Как-то долго.

В rabin-karp придумали оптмизировать алгоритм за счёт rolling hash. 
Давайте посчитаем хэш паттерна и хэш первых m символов нашей строки, где m - длина паттерна.

![alt text](<Снимок экрана 2025-12-15 в 20.15.37.png>)

Дальше хэш паттерн фиксирован, а хэш каждой новой подстроки длины m получается удалением самого левого символа и прибавлением нового правого.

![alt text](<Снимок экрана 2025-12-15 в 20.40.01.png>)

Т.е идея rolling hash в том, что мы не вычисляем хэш новой подстроки каждый раз заново, а просто удаляем данные самого левого символа и прибавляем данные нового правого.

**Вопрос:** а что за данные собственно?
Тут самое интересное - зависит от того, что за хэш функцию мы используем. т.е по сути это может быть много что, но нам выгоднее сделать хэш таким, чтобы он давал как можно меньше коллизий.
Например, я использовала в алгоритме вот такой:

![alt text](<Снимок экрана 2025-12-15 в 20.46.07.png>)
где s_0...s_(k-1) - ascii коды символов, p-основание системы счисления, m - просто большое число, по которому берётся остаток, чтобы за int не перевалить.
На каждом шаге мы убираем самый левый символ * p^(k-1). Сдвигаем степени, чтобы слева была вновь самая старшая, и прибавляем новый правый символ.


### Оценка сложности итогового алгоритма
Самое интересное, что у алгоритма нет конкретной сложности, так как всё сильно зависит от выбранной хэш функции.
Давайте разберёмся. У алгоритма есть блоки кода с фиксированной сложностью, и наоборот.

**Фиксированная сложность**
Ниже представлены шаги расчёта хэша для паттерна и первых m символов подстроки. Они строго зависят от m. Получаем тут линейную сложность O(m). 
```
for i in range(m):
  hash_pattern = (hash_pattern * p + ord(pattern[i])) % mod
  hash_window = (hash_window * p + ord(text[i])) % mod
```


**Нефиксированная сложность**
Ниже код, в котором происходит следующее:
1. Сравнение значений хэшей. Это просто два числа, так что O(1)
2. Если хэши совпали, то сравнение строк, а это уже O(m)
3. Сдвиг окна. Тут просто хитрая работа с числами, поэтому O(1)
```
for i in range(n - m + 1):
  # если хэши совпали — проверяем посимвольно
  if hash_window == hash_pattern:
      if text[i:i + m] == pattern:
          result.append(i)

  # rolling hash: сдвиг окна
  if i < n - m:
      # убираем левый символ
      hash_window = (hash_window - ord(text[i]) * power) % mod
      # сдвигаем степени
      hash_window = (hash_window * p) % mod
      # добавляем новый символ
      hash_window = (hash_window + ord(text[i + m])) % mod
```

Глобально, вроде всё круто. НО вот если наша хэш функция плохая, то хэши будут часто совпадать, а значит мы можем практически на каждом шаге цикла делать O(m) сравнений, тем самым получая сложность O(n*m).
**Что может быть за плохая хэш функция?**
Например, сумма ascii кодов строки. Её проблема в том, что hash(aba) = hash(baa) = hash(aab).

**Итог**
Если выбрать хорошую хэш функцию, то сложность будет в районе O(n). Но а если выбрать плохую, то O(n*m)